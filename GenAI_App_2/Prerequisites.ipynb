{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OBJECT DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Theory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to install Open CV for onbject detection?\n",
    "\n",
    "* pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the use of Open CV library?\n",
    "\n",
    "* image processing\n",
    "* computer vision tasks like object detection\n",
    "* image recognition\n",
    "* video analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the significance of the DNN module in Open CV library?\n",
    "\n",
    "* it is used for deep learning\n",
    "* allows integration of pre-trained deep learning models like YOLOv4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Name the model we are going to use for object detection.\n",
    "\n",
    "* YOLO (You Only Look Once)\n",
    "* state-of-the-art object detection model\n",
    "* its known for its speed and accuracy\n",
    "* YOLOv4 is a specific version of YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the algorithm behind the above object detection model?\n",
    "\n",
    "Algorithms involved behind YOLO model is :\n",
    "1. Deep neural networks\n",
    "2. Convolutional Neural Network ; backbone of YOLOv4 designed to process visual data efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** List out the key concepts and jargons related to YOLO model.\n",
    "\n",
    "1. **Grid Cells** - The image is divided into a grid of cells.\n",
    "2. **Bounding Box Prediction** - Each grid cell predicts a fixed number of bounding boxes, along with their confidence scores.\n",
    "3. **Class Probability** - Each bounding box is associated with class probabilities, indicating the likelihood of different object classes (e.g., person, car, dog).\n",
    "4. **Non-Maximum Suppression (NMS)** - A technique to eliminate redundant detections and select the most confident one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give a brief about the YOLO's Deep-CNN architecture.\n",
    "\n",
    "1. **Backbone Network** :\n",
    "\n",
    "* core of the network\n",
    "* responsible for extracting features from the input image\n",
    "* typically employs a pre-trained network like Darknet-53 or EfficientNet.\n",
    "* extract rich feature maps at different scales\n",
    "* captures both fine-grained and coarse-grained information\n",
    "* extracts fundamental features from the image, such as edges, textures, and shapes\n",
    "\n",
    "2. **Neck** :\n",
    "\n",
    "* responsible for fusing feature maps from different layers of the backbone\n",
    "* crucial for object detection at various scales\n",
    "* techniques like Feature Pyramid Networks (FPNs) are commonly used to achieve this\n",
    "* combines feature maps from different layers to enhance feature representation at various scales\n",
    "* crucial for detecting objects of different sizes\n",
    "\n",
    "3. **Head** :\n",
    "\n",
    "* final layer of the network\n",
    "* actual object detection predictions are made here\n",
    "* consists of multiple prediction layers\n",
    "* each layer is responsible for predicting bounding boxes and class probabilities for a specific scale\n",
    "* Each prediction layer outputs:\n",
    "    1. Bounding box coordinates (x, y, width, height)\n",
    "    2. Objectness score (confidence that there is an object in the box)\n",
    "    3. Class probabilities for different object categories\n",
    "* Predicts the final output, including bounding box coordinates and class probabilities\n",
    "* ensures that the model can accurately locate and classify objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Give a step-by-step guide related to the working of YOLO.\n",
    "\n",
    "1. **Image Division** : \n",
    "    * input image is divided into a grid of cells\n",
    "    * like a chessboard\n",
    "    * each cell in this grid is responsible for detecting objects that fall within its boundaries\n",
    "\n",
    "2. **Feature Extraction** : \n",
    "    * A CNN extracts features from the image\n",
    "\n",
    "3. **Bounding Box Prediction** : \n",
    "    * For each grid cell, the network predicts bounding boxes and their associated class probabilities\n",
    "    * each cell will predict a certain number of bounding boxes (usually 3 or more)\n",
    "    * These boxes are rectangles that can potentially enclose objects within the cell\n",
    "    * For each predicted bounding box, the network will assign probabilities to different object classes (e.g., person, car, dog)\n",
    "    * probabilities indicate the likelihood of the object being of that particular class\n",
    "    * So, for a single cell, the network might predict something like:\n",
    "        (i) Bounding Box 1: Confidence: 0.7, Class Probabilities: Person (0.8), Car (0.2)\n",
    "        (ii) Bounding Box 2: Confidence: 0.3, Class Probabilities: Bicycle (0.6), Motorcycle (0.4)\n",
    "    * network does this for every cell in the grid\n",
    "\n",
    "4. **NMS** :\n",
    "    * After all predictions are made, a technique called Non-Maximum Suppression (NMS) is applied to eliminate redundant detections and select the most confident ones\n",
    "    * overlapping bounding boxes with low confidence scores are removed, leaving only the most confident detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Certain files should be downloaded into the project directory for object detection using Open CV's YOLO. Name those files.\n",
    "\n",
    "* yolov4.weights\n",
    "* yolov4.cfg\n",
    "* coco.names\n",
    "\n",
    "When you load the model using cv2.dnn.readNet, the function needs to locate these files to load the network architecture and weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What are the contents of yolov4.weights file?\n",
    "\n",
    "* file contains the trained weights of the YOLOv4 model\n",
    "* weights are learned parameters that allow the model to accurately detect objects in images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What about yolov4.cfg file ?\n",
    "\n",
    "* contains the configuration of the YOLOv4 model's architecture\n",
    "* defines the network layers, their parameters, and the overall structure of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What about coco.names?\n",
    "\n",
    "* contains a list of object classes that the YOLOv4 model is trained to detect\n",
    "* classes are typically from the COCO dataset, which includes objects like people, cars, bicycles, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the 1st and formost step to implement object detection using Open CV's YOLO?\n",
    "\n",
    "* Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import cv2\n",
    "\n",
    "# load the 'YOLO' model\n",
    "net = cv2.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **cv2.dnn**-\n",
    "  - refers to OpenCV's Deep Neural Network (DNN) module\n",
    "  - allows loading pre-trained neural networks, making predictions, and performing various deep learning tasks\n",
    "\n",
    "* **readNet**: \n",
    "  - a method \n",
    "  - loads the pre-trained weights and the configuration file for a neural network model\n",
    "\n",
    "* model's structure (from the .cfg file) and its learned parameters (from the .weights file) are loaded into the net object\n",
    "* net object is now ready to perform object detection on images or video frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the next step? What is its significance?\n",
    "\n",
    "* Loading class labels step\n",
    "\n",
    "```python\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# classes will be a list like\n",
    "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', ...]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **open('coco.names', 'r')** - Opens the coco.names file in read mode ('r')\n",
    "* **f.readlines()** - \n",
    "   - Reads all the lines from the file into a list\n",
    "   - here, each line in the file corresponds to a class name\n",
    "* **[line.strip() for line in f.readlines()]** -\n",
    "   - a list comprehension \n",
    "   - removes any leading or trailing whitespace/newline characters from each line in the list of lines\n",
    "\n",
    "* such a list is created to map each detected object's predicted class ID to a human-readable label\n",
    "* YOLO predicts class IDs as numerical indices (e.g., 0 for \"person,\" 1 for \"bicycle\")\n",
    "* coco.names file provides the corresponding class names\n",
    "* so, meaningful results instead of numeric IDs during detection can be displayed \n",
    "* Without this step, the output would only show IDs, which would be unintuitive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to access the layers of the deep net we have initiated previously? Why is it even necessary?\n",
    "\n",
    "* Retrieving Layer Names Step\n",
    "```python\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# identifying output layers\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Getting layer names are important because** :\n",
    "   - To access the names of all layers in the neural network.\n",
    "   - YOLO's neural network consists of many layers, but not all are relevant for final output.\n",
    "   - Understanding the architecture is crucial to identifying the layers responsible for producing predictions\n",
    "   - This step sets up the groundwork for identifying which layers are output layers\n",
    "   - **net.getLayerNames()**\n",
    "      * Returns a list of all the layer names in the YOLOv4 network.\n",
    "      * These names represent different computational layers in the neural network.\n",
    "\n",
    "* **net.getUnconnectedOutLayers()**\n",
    "  - Returns the indices of the output layers\n",
    "  - output layers = layers that produce the final predictions\n",
    "  - YOLOv4 typically has 3 output layers to predict bounding boxes at different scales.\n",
    "* **[i[0] - 1 for i in ...]** \n",
    "  - Each i is an index starting from 1\n",
    "  - Subtracting 1 converts it to a 0-based index, which aligns with Python’s indexing\n",
    "* **[layer_names[i[0] - 1] for i in ...]** :\n",
    "  - Retrieves the actual names of the output layers using the adjusted indices\n",
    "\n",
    "* **Identifying output layers are necessary because** :\n",
    "  - To determine which layers produce the final detection results (bounding boxes, confidence scores, and class probabilities)\n",
    "  - YOLOv4 outputs predictions from specific layers, typically three for multi-scale detection.\n",
    "  - Correctly identifying these layers ensures that the object detection results are extracted and processed accurately\n",
    "  - Without this step, the model would not know where to retrieve the detection data, making it impossible to generate predictions\n",
    "  - If the output layers like ['yolo_82', 'yolo_94', 'yolo_106'] are not identified, the forward pass will not produce the necessary output for object detection.\n",
    "  - ensures that detection results are extracted from the correct layers in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** What is the next step?\n",
    "\n",
    "* Loading the image step\n",
    "```python\n",
    "# Convert the uploaded file (binary data) into a NumPy array of unsigned 8-bit integers (image bytes)\n",
    "# Why? --> OpenCV requires the image data in this format for decoding\n",
    "# bytearray --> Converts the binary data into a mutable sequence of bytes\n",
    "# np.asarray --> Creates a NumPy array from the byte sequence\n",
    "file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
    "\n",
    "# Decode the image bytes into an OpenCV image format (BGR color format)\n",
    "# OpenCV can only process images in a specific format\n",
    "# this function transforms raw bytes into an image matrix\n",
    "img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n",
    "\n",
    "# Retrieve the height, width, and number of color channels from the image's shape\n",
    "# _ is the placeholder for the 3rd value, representing the number of color channels (typically 3 for BGR)\n",
    "height, width, _ = img.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Should there be an image preprocessing step before actual object detection? If yes, how to do it?\n",
    "\n",
    "* Yes, \n",
    "\n",
    "```python\n",
    "# YOLO Processing\n",
    "blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**blob = cv2.dnn.blobFromImage(...)**\n",
    "  - Converts the input image into a \"blob\" that YOLO can process\n",
    "  - Transforms the image into a 4D blob, which is a standard format for deep learning models in OpenCV\n",
    "  - A 4D blob with shape (1, 3, 416, 416) representing (batch_size, channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** It seems there are a lot of parameters to be decided during the blob making step. What does each represetn?\n",
    "\n",
    "||Parameter|Purpose|\n",
    "|----|----|---|\n",
    "|1|img|The input image to be processed (loaded earlier)|\n",
    "|2|0.00392|A scaling factor (1/255), used to normalize pixel values from [0, 255] to [0, 1]|\n",
    "|3|(416, 416)|The spatial size to which the image is resized (YOLOv4 requires this specific input size)|\n",
    "|4|(0, 0, 0)|Mean subtraction values (used for normalization; here, no mean subtraction is applied)|\n",
    "|5|True|Indicates swapping of the R and B channels (OpenCV uses BGR by default, YOLO expects RGB)|\n",
    "|6|crop=False|Ensures the image is resized without cropping|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to feed the deep YOLO net with the processed image?\n",
    "\n",
    "```python\n",
    "net.setInput(blob)\n",
    "```\n",
    "This step loads the blob into the YOLO network, preparing it for the forward pass, where the network will perform object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How to get output from the net corresponding to the input?\n",
    "```python\n",
    "outs = net.forward(output_layers)\n",
    "```\n",
    "* **net.forward()** - Executes the forward pass and returns the detection results from the specified output layers.\n",
    "* **output_layers** - Contains the names of the YOLO output layers (e.g., ['yolo_82', 'yolo_94', 'yolo_106'])\n",
    "* YOLO's predictions (bounding boxes, class scores, and confidence scores) are generated in the forward pass\n",
    "* This line collects all the raw output needed for further processing \n",
    "* outs is a list of NumPy arrays, each containing detection data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** How is a single detection data organised within an array?\n",
    "\n",
    "* Each array contains predictions in the format:\n",
    "\n",
    "**[center_x,  center_y,  width,  height,  confidence, class_prob_0, class_prob_1, ...]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** For each grid cell, the layer might have predicted several of bounding boxes, their confidence scores and labels(class id). How do we get all these specific informations from outs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# initialize empty lists to collect the specific informations regarding detected objects\n",
    "boxes, confidences, class_ids = [], [], []\n",
    "\n",
    "# iterate through the list of arrays -> outs\n",
    "# outs contains multiple arrays, each corresponding to a YOLO output layer\n",
    "# iterating through results of each output layer\n",
    "for out in outs:\n",
    "    # Each array in outs contains detections from all grid cells for that specific output layer\n",
    "    # iterating over all detections from a single output layer\n",
    "    for detection in out:\n",
    "        # extracting all class probabilities\n",
    "        scores = detection[5:]\n",
    "        # selecting the class_id with maximum confidence score\n",
    "        class_id = np.argmax(scores)\n",
    "        # getting that maximum confidence score value\n",
    "        confidence = scores[class_id]\n",
    "        # thresholding/ filtering to capture confident detections only\n",
    "        if confidence > 0.5:\n",
    "            # bounding box coordinates occur in normalized format ([0,1] range relative to the image size)\n",
    "            # so center of the bounding box, scaled by the image's width and height\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            # Width (w) and height (h) of the bounding box, scaled by image size\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "            # Top-left corner of the bounding box \n",
    "            # calculated by shifting from the center\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "        boxes.append([x, y, w, h])\n",
    "        confidences.append(float(confidence))\n",
    "        class_ids.append(class_id)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Some bounding boxes may be overlapping and redundant. How to remove those?\n",
    "```python\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "```\n",
    "* indexes is a list of indices of the selected boxes after NMS is applied\n",
    "* NMS is used to eliminate multiple bounding boxes for the same object\n",
    "* It keeps the box with the highest confidence score and removes others that overlap significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** NMSBoxes function has many parameters. What does each indicate?\n",
    "\n",
    "||Parameter|Purpose|\n",
    "|---|----|---|\n",
    "|1|boxes|List of bounding box coordinates (in [x, y, w, h] format)|\n",
    "|2|confidences|List of confidence scores for each box (indicating how confident YOLO is about the presence of an object)|\n",
    "|3|0.5| Minimum confidence threshold (detections with a confidence below this value are ignored)|\n",
    "|4|0.4|Overlap threshold (if the intersection over union (IoU) of two boxes exceeds this threshold, one of them will be suppressed)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** All NMS selected boxes will have class ids. How to map those class ids to respective class labels?\n",
    "```python\n",
    "# initialize an empty list to store label's of detected objects\n",
    "detected_objects = []\n",
    "\n",
    "# iterating through the list of bounding boxes\n",
    "for i in range(len(boxes)):\n",
    "    # checking if the box is NMS selected or not\n",
    "    if i in indexes:\n",
    "        # extracting all cordinates of that box\n",
    "        x, y, w, h = boxes[i]\n",
    "        # get class_id of that box\n",
    "        # using that id, get class label from list of class labels\n",
    "        label = str(classes[class_ids[i]])\n",
    "        # storing the label\n",
    "        detected_objects.append(label)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q :** Now how to highlight the detected objects by drawing bounding boxes and labels?\n",
    "```python\n",
    "\n",
    "# part of the previous for loop\n",
    "cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "```"
   ]
  }],
 "metadata": {
  "kernelspec": {
   "display_name": "web_app_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
